{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c24eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\dip_env\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\nguye\\anaconda3\\envs\\dip_env\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 701ms/step - accuracy: 0.3870 - loss: 6.1026 - val_accuracy: 0.1182 - val_loss: 1.3441\n",
      "Epoch 2/50\n",
      "\u001b[1m 1/13\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 676ms/step - accuracy: 0.3750 - loss: 2.9916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\dip_env\\lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.3750 - loss: 2.9916 - val_accuracy: 0.1818 - val_loss: 1.2736\n",
      "Epoch 3/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 646ms/step - accuracy: 0.4503 - loss: 2.5837 - val_accuracy: 0.2727 - val_loss: 3.5649\n",
      "Epoch 4/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2812 - loss: 1.6069 - val_accuracy: 0.2727 - val_loss: 4.0664\n",
      "Epoch 5/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 642ms/step - accuracy: 0.4516 - loss: 1.4329 - val_accuracy: 0.2727 - val_loss: 9.2493\n",
      "Epoch 6/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.4688 - loss: 1.3118 - val_accuracy: 0.2727 - val_loss: 9.6637\n",
      "Epoch 7/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 632ms/step - accuracy: 0.5014 - loss: 1.2587 - val_accuracy: 0.2727 - val_loss: 14.7513\n",
      "Epoch 8/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5312 - loss: 1.3151 - val_accuracy: 0.2727 - val_loss: 14.8322\n",
      "Epoch 9/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 664ms/step - accuracy: 0.5711 - loss: 1.1473 - val_accuracy: 0.2727 - val_loss: 14.4867\n",
      "Epoch 10/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.5312 - loss: 1.1363 - val_accuracy: 0.2727 - val_loss: 14.3263\n",
      "Epoch 11/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 679ms/step - accuracy: 0.5517 - loss: 1.0495 - val_accuracy: 0.2727 - val_loss: 12.2939\n",
      "Epoch 12/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.6250 - loss: 1.0076 - val_accuracy: 0.2727 - val_loss: 12.0992\n",
      "Epoch 13/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 661ms/step - accuracy: 0.5652 - loss: 1.0699 - val_accuracy: 0.2727 - val_loss: 8.7709\n",
      "Epoch 14/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5312 - loss: 1.0841 - val_accuracy: 0.2727 - val_loss: 8.5652\n",
      "Epoch 15/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 664ms/step - accuracy: 0.5564 - loss: 1.0437 - val_accuracy: 0.2727 - val_loss: 6.2133\n",
      "Epoch 16/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.9935 - val_accuracy: 0.2727 - val_loss: 6.0258\n",
      "Epoch 17/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 671ms/step - accuracy: 0.5742 - loss: 1.0370 - val_accuracy: 0.2727 - val_loss: 4.0126\n",
      "Epoch 18/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.3438 - loss: 1.1473 - val_accuracy: 0.2727 - val_loss: 3.9361\n",
      "Epoch 19/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 679ms/step - accuracy: 0.5879 - loss: 0.9993 - val_accuracy: 0.2545 - val_loss: 4.0766\n",
      "Epoch 20/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.5625 - loss: 1.1930 - val_accuracy: 0.2545 - val_loss: 4.1121\n",
      "Epoch 21/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 668ms/step - accuracy: 0.5498 - loss: 1.0374 - val_accuracy: 0.2364 - val_loss: 3.5493\n",
      "Epoch 22/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.6562 - loss: 1.0458 - val_accuracy: 0.2182 - val_loss: 3.5815\n",
      "Epoch 23/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 677ms/step - accuracy: 0.5080 - loss: 1.1552 - val_accuracy: 0.2727 - val_loss: 6.9758\n",
      "Epoch 24/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.7500 - loss: 0.9773 - val_accuracy: 0.2727 - val_loss: 7.3242\n",
      "Epoch 25/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 682ms/step - accuracy: 0.5655 - loss: 1.0435 - val_accuracy: 0.2727 - val_loss: 8.4699\n",
      "Epoch 26/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7000 - loss: 1.1029 - val_accuracy: 0.2727 - val_loss: 8.4722\n",
      "Epoch 27/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 640ms/step - accuracy: 0.5946 - loss: 1.0064 - val_accuracy: 0.2727 - val_loss: 7.7937\n",
      "Epoch 28/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6875 - loss: 0.8613 - val_accuracy: 0.2727 - val_loss: 7.7556\n",
      "Epoch 29/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 635ms/step - accuracy: 0.5842 - loss: 1.0438 - val_accuracy: 0.2636 - val_loss: 5.3995\n",
      "Epoch 30/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.4688 - loss: 1.0412 - val_accuracy: 0.2636 - val_loss: 5.2862\n",
      "Epoch 31/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 661ms/step - accuracy: 0.5709 - loss: 1.0527 - val_accuracy: 0.4091 - val_loss: 1.8603\n",
      "Epoch 32/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.6875 - loss: 0.9760 - val_accuracy: 0.4545 - val_loss: 1.8675\n",
      "Epoch 33/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 634ms/step - accuracy: 0.5241 - loss: 1.0749 - val_accuracy: 0.4000 - val_loss: 1.8409\n",
      "Epoch 34/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5938 - loss: 1.0912 - val_accuracy: 0.4545 - val_loss: 1.8440\n",
      "Epoch 35/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 648ms/step - accuracy: 0.5602 - loss: 1.0372 - val_accuracy: 0.2545 - val_loss: 1.6932\n",
      "Epoch 36/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5938 - loss: 1.0297 - val_accuracy: 0.2636 - val_loss: 1.7037\n",
      "Epoch 37/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 629ms/step - accuracy: 0.5916 - loss: 0.9503 - val_accuracy: 0.2636 - val_loss: 1.7655\n",
      "Epoch 38/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5625 - loss: 0.8764 - val_accuracy: 0.2818 - val_loss: 1.7323\n",
      "Epoch 39/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 664ms/step - accuracy: 0.5501 - loss: 1.0572 - val_accuracy: 0.3727 - val_loss: 1.2857\n",
      "Epoch 40/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.5000 - loss: 1.0571 - val_accuracy: 0.3545 - val_loss: 1.2758\n",
      "Epoch 41/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 661ms/step - accuracy: 0.5436 - loss: 1.0032 - val_accuracy: 0.3091 - val_loss: 1.5472\n",
      "Epoch 42/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.7188 - loss: 0.8370 - val_accuracy: 0.2818 - val_loss: 1.6285\n",
      "Epoch 43/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 669ms/step - accuracy: 0.5968 - loss: 0.9600 - val_accuracy: 0.2727 - val_loss: 2.5419\n",
      "Epoch 44/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.4688 - loss: 1.4592 - val_accuracy: 0.2727 - val_loss: 2.6817\n",
      "Epoch 45/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 664ms/step - accuracy: 0.5181 - loss: 1.0510 - val_accuracy: 0.2727 - val_loss: 2.2922\n",
      "Epoch 46/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.5625 - loss: 1.1091 - val_accuracy: 0.2727 - val_loss: 2.1669\n",
      "Epoch 47/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 647ms/step - accuracy: 0.5505 - loss: 0.9976 - val_accuracy: 0.2818 - val_loss: 1.6438\n",
      "Epoch 48/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.5312 - loss: 1.1156 - val_accuracy: 0.2818 - val_loss: 1.5972\n",
      "Epoch 49/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 654ms/step - accuracy: 0.5729 - loss: 0.9755 - val_accuracy: 0.3000 - val_loss: 1.4859\n",
      "Epoch 50/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.4500 - loss: 1.0446 - val_accuracy: 0.3000 - val_loss: 1.4901\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nguye\\AppData\\Local\\Temp\\tmpgu9cch_a\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nguye\\AppData\\Local\\Temp\\tmpgu9cch_a\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\nguye\\AppData\\Local\\Temp\\tmpgu9cch_a'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor_172')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2019313534224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291713200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291717776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291719536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291716896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291718304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2020614039056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291720416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291722176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291721648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291721824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291723056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019291723936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019335443152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019335439632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019335443680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019335444912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019335442448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019335451776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019335453536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019335450016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2019335452656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hàm tải dữ liệu từ thư mục ảnh\n",
    "def load_data(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            label = filename.split(\"_\")[0]\n",
    "            if label in [\"right\", \"forward\", \"left\"]:\n",
    "                img = cv2.imread(os.path.join(data_dir, filename))\n",
    "                img = cv2.resize(img, (128, 128))  # Resize về kích thước 128x128\n",
    "                images.append(img)\n",
    "                labels.append([\"right\", \"forward\", \"left\"].index(label))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Tải và chia dữ liệu\n",
    "data_dir = \"dataset\"  # Thay bằng đường dẫn thực tế\n",
    "images, labels = load_data(data_dir)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    images, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Chuẩn hóa dữ liệu (giá trị pixel từ [0,255] về [0,1])\n",
    "train_images = train_images / 255.0\n",
    "val_images = val_images / 255.0\n",
    "\n",
    "# Tăng cường dữ liệu để chống overfitting\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "train_generator = train_datagen.flow(train_images, train_labels, batch_size=32)\n",
    "\n",
    "# Xây dựng mô hình CNN\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(128, 128, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),  # Dropout để chống overfitting\n",
    "    layers.Dense(3, activation=\"softmax\")  # 3 lớp: right, forward, left\n",
    "])\n",
    "\n",
    "# Compile mô hình\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    steps_per_epoch=len(train_images) // 32\n",
    ")\n",
    "\n",
    "# Vẽ đồ thị loss\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"cnn_loss_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# Lưu mô hình dưới dạng .tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(\"cnn_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tải mô hình .tflite\n",
    "interpreter = tf.lite.Interpreter(model_path=\"cnn_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Lấy thông tin input và output\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Kiểm tra trên luồng video\n",
    "stream_url = \"http://192.168.46.130:81/stream\"\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Không thể đọc luồng video.\")\n",
    "        break\n",
    "\n",
    "    # Chuẩn bị frame\n",
    "    frame_resized = cv2.resize(frame, (128, 128))\n",
    "    frame_processed = frame_resized / 255.0\n",
    "    input_data = np.expand_dims(frame_processed, axis=0).astype(np.float32)\n",
    "\n",
    "    # Chạy dự đoán\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Lấy kết quả\n",
    "    output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "    prediction = np.argmax(output_data[0])\n",
    "    result = [\"right\", \"forward\", \"left\"][prediction]\n",
    "    print(f\"Dự đoán: {result}\")\n",
    "\n",
    "    # Hiển thị frame (tùy chọn)\n",
    "    cv2.imshow(\"Video Stream\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):  # Nhấn 'q' để thoát\n",
    "        break\n",
    "\n",
    "# Giải phóng tài nguyên\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
